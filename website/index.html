<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Story-Adapter</title>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <link rel="stylesheet" href="styles.css">
    <link href="../css/cs.css" rel="stylesheet">

    <!-- <script type="text/javascript" src="scroll.js"></script> -->

     <style>
      body {
        background: #fdfcf9 no-repeat fixed top left;
        font-family:'DM Mono','Open Sans', sans-serif;
      }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <h1>Story-Adapter: A Training-free Iterative Framework</h1>
            <br/> 
            <br/>
            <br/>
            <h1> for Long Story Visualization</h1>
            <br/> 
            <br/>
            <br/>
            <h1>Visualization</h1>
            <br/>
            <br/>
        
            <div class="details">
                <a href="https://github.com/jwmao1" target="_blank">Jiawei Mao</a> <sup>* 1,2</sup>, 
                <a href="https://xk-huang.github.io" target="_blank">Xiaoke Huang</a> <sup>* 1</sup>, 
                <a href="https://yunfeixie233.github.io/" target="_blank">Yunfei Xie</a> <sup>1</sup>, 
                <a href="" target="_blank">Yuanqi Chang</a> <sup>2</sup>, 
                <a href="https://thefllood.github.io/mudehui.github.io/ target="_blank">Mude Hui</a> <sup>1</sup>, 
                <a href="https://scholar.google.com.sg/citations?user=trbIWoQAAAAJ&hl=zh-CN" target="_blank">Bingjie Xu</a> <sup>3</sup>, 
                <a href="https://yuyinzhou.github.io/" target="_blank">Yuyin Zhou</a> <sup>1</sup>
            </div>
            <div class="details">
                <sup>1</sup><a href="https://ucsc-vlaa.github.io/" target="_blank">UC Santa Cruz</a>, 
                <sup>2</sup><a href="https://en.hdu.edu.cn/" target="_blank">Hangzhou Dianzi University</a>, 
                <sup>3</sup><a href="https://www.singaporetech.edu.sg/" target="_blank">Singapore Institute of Technology</a>
            </div>
            <div class="details"><sup>*</sup>Equal Contribution</div>
            <div class="links">
                <a href="https://github.com/aim-uofa/MovieDreamer" target="_blank">
                    <i class="fab fa-github"></i> GitHub
                </a>
                <a href="https://arxiv.org/abs/2407.16655" target="_blank">
                    <i class="fas fa-file-alt"></i> arXiv
                </a>
            </div>
        </div>
        

        
               <div class="column">
                 
               <audio controls>
   <source src="MovieDreamer.mp3" type="audio/mpeg">
  Your browser does not support the audio tag.
</audio>
                 
               </div>
              


    </header>
    <div class="container main">
        <section class="section section-abstract">
            <div class="add-div" style="height: 150px;">
                <image src="images/logo2.png" height="150"/>
            </div>
            <h2>Abstract</h2>
            <div class="abs">Generative models have demonstrated promising results in open-ended visual storytelling. However, most mainstream methods employ the autoregressive generative framework, which lacks a global comprehension of the story and is prone to error accumulations. Those issues become more pronounced as the length of the storytelling increases. We present Story-Adapter, a simple and efficient training-free adaptation of text-to-image diffusion models for open-ended ultra-long story generation. We propose Iterative Consistent Attention (ICA) scheme to refine the outputs in loops: 
                Based on the initial visual contents from the diffusion model, we iteratively apply the reference images attention in ICA to build a global comprehension of the story, refine the consistency with the references, and preserve the semantics from the text prompts.Specifically, in each step, we leverage all the outputs from the previous iteration as reference tokens to guide the attention matrices update in a training-free manner. Compared with the previous story visualization arts, Story-Adapter attains better multi-character consistency and visual quality both qualitatively and quantitatively, yet enjoys better efficiency during generation. 
            </div>
        </section>

        <section class="section section-other">
            <h2>Story-Adapter Architecture</h2>
            <div class="abs">Story-Adapter pipeline. Firstly, Story-Adapter generates a series of image sequences based only on the textual descriptors in the story as Story-Adapter initialization to provide reference images for the first iteration of Story-Adapter looped stabilization diffusion framework. Then Reference Images Cross Attention provides Story-Adapter with a global comprehension of the story based on all the reference images, aggregating similar visual representations between the generated image and all the reference images to maintain consistency in the story visualization. Finally, in Story-Adapter's loop stabilization diffusion framework, Story-Adapter uses the result of the previous iteration as the reference images for the next iteration. Visualization results of Story-Adapter are gradually improved through iteration.</div>
            <!-- <div class="titanic-title">
                <image src="images/TITANIC.png" width="150"/>
            </div> -->

            <div class="add-div" style="height: 515px;">
                <image src="images/images/Story-Adapter Architecture.png" height="515" style="width: 100%;"/>
            </div>
        </section>
        <section class="section section-other">
            <h2>Qualitative Comparison of StorySalon Story</h2>
            <div class="abs">Qualitative comparison of story visualization shows AR-LDM and StoryGen generate coherent image sequences but degrade with story length due to autoregressive errors. StoryDiffusion and Story-Adapter perform well, though StoryDiffusion struggles with subject consistency and ID image flaws due to high computation demands. Story-Adapter better meets the requirements for effective story visualization.</div>
            <!-- <div class="titanic-title">
                <image src="images/TITANIC.png" width="150"/>
            </div> -->

            <div class="add-div" style="height: 670px;">
                <image src="images/images/Qualitative Comparison of StorySalon Story.png" height="670" style="width: 100%;"/>
            </div>
        </section>
        <section class="section section-other">
            <h2>Qualitative Comparison of Ultra-Long Story</h2>
            <div class="carousel">
                <div class="carousel-inner">
                    <img src="images/images/compare1/compare/story-adapter/comic1.png">
                    <img src="images/images/compare1/compare/storydiffusion/comic17.png">
                    <img src="images/images/compare1/compare/storygen/comic17.png">
                </div>
                <div class="carousel-buttons">
                    <button id="prev">❮</button>
                    <button id="next">❯</button>
                </div>
            </div>

        </section>
        <section class="section section-other">
            <h2>StorySalon Story Result</h2>
            <!-- <div class="abs">MovieDreamer is <b>ORTHOGONAL</b> to existing long video generation methods, but benefits from them. </div>
            <div class="abs">
                Existing long video generation methods typically focus on generating a long video clip with one image or text as input, ensuring high-quality results of tens of seconds. However, it is extremely computational intensive to scale them up to generate a long video of minutes, and almost impossible for hours. We address this problem from a different perspective, namely, by generating long videos in a hierarchical way. Specifically, we first generate keyframes, which serve as anchor frames to generate the long video. Moreover, our paradigm unifies long story generation and long video generation. Firstly, we surpass the existing methods in terms of the length of the generated content, both in story and video generation, while ensuring no degradation in quality. Secondly, our generation quality also exceeds the current state-of-the-art methods, which is demonstrated in the evaluation metrics. Lastly, our method is highly flexible, allowing the use of some of the current high-quality closed-source video generation models to create exceptionally high-quality long videos with rich narrative, with multiple-character consistency well-preserved.
            </div>

            <h3>MovieDreamer + Luma</h3> -->
            <div class="video-grid" style="margin-bottom: 8px;">
                <img src="images/images/gif/our_000026.gif" style="width: 100%;">
                <img src="images/images/gif/our_000362.gif" style="width: 100%;">
                <img src="images/images/gif/our_004542.gif" style="width: 100%;">
            </div>
           
            <div class="video-grid" style="margin-bottom: 8px;">
                <img src="images/images/gif/our_005169.gif" style="width: 100%;">
                <img src="images/images/gif/our_005332.gif" style="width: 100%;">
                <img src="images/images/gif/our_005717.gif" style="width: 100%;">
            </div>
            
            <div class="video-grid" style="margin-bottom: 8px;">
                <img src="images/images/gif/our_005934.gif" style="width: 100%;">
                <img src="images/images/gif/our_007137.gif" style="width: 100%;">
                <img src="images/images/gif/our_007389.gif" style="width: 100%;">
            </div>
        </section>


        <section class="section section-other">
            <h2>50 Ultra-Long Story Result</h2>
            <!-- <div class="abs">50 Ultra-Long Story Result</div> -->
            <!-- <div class="abs">Story-Adapter architecture overview. To visualize ultra-long stories, we designed iterative consistent attention (ICA) in a pre-trained text-to-image diffusion model. We first employ a stable diffusion (SD) model guided only by text descriptors to create rich visual content as an initialization step for Story-Adapter to provide reference images to preserve the textual controllability of SD. Based on CLIP's token simplification, ICA efficiently establishes a global comprehension of the story for Story-Adapter based on the visual content of all the reference tokens in the ultra-long story to aggregate similar visual representations among all the reference images during the generation process to maintain the consistency of the image sequences in the ultra-long story visualization. On the basis of global comprehension, Story-Adapter iteratively applies reference image cross attention in ICA in looped stable diffusion framework to reduce differences among reference images resulting from more characters and more complex interactions in ultra-long stories.</div> -->
            <div class="carousel1">
                <div class="carousel-inner1">
                    <img src="images/images/50_comic/comic1.png" style="background-size: cover;">
                    <img src="images/images/50_comic/comic2.png" style="background-size: cover;">
                    <img src="images/images/50_comic/comic4.png" style="background-size: cover;">
                    <img src="images/images/50_comic/comic5.png" style="background-size: cover;">
                    <img src="images/images/teaser.png" style="background-size: cover;">                    
                </div>
                <div class="carousel-buttons">
                    <button id="prev1">❮</button>
                    <button id="next1">❯</button>
                </div>
            </div>

        </section>



        <section class="section section-other">
            <h2>100 Ultra-Long Story Result</h2>
            <!-- <div class="abs">50 Ultra-Long Story Result</div> -->
            <!-- <div class="abs">Story-Adapter architecture overview. To visualize ultra-long stories, we designed iterative consistent attention (ICA) in a pre-trained text-to-image diffusion model. We first employ a stable diffusion (SD) model guided only by text descriptors to create rich visual content as an initialization step for Story-Adapter to provide reference images to preserve the textual controllability of SD. Based on CLIP's token simplification, ICA efficiently establishes a global comprehension of the story for Story-Adapter based on the visual content of all the reference tokens in the ultra-long story to aggregate similar visual representations among all the reference images during the generation process to maintain the consistency of the image sequences in the ultra-long story visualization. On the basis of global comprehension, Story-Adapter iteratively applies reference image cross attention in ICA in looped stable diffusion framework to reduce differences among reference images resulting from more characters and more complex interactions in ultra-long stories.</div> -->
            <div class="carousel2">
                <div class="carousel-inner2">
                    <img src="images/images/50_comic/comic1.png" style="background-size: cover;">
                    <img src="images/images/50_comic/comic2.png" style="background-size: cover;">
                    <img src="images/images/50_comic/comic3.png" style="background-size: cover;">
                    <img src="images/images/50_comic/comic4.png" style="background-size: cover;">
                    <img src="images/images/50_comic/comic5.png" style="background-size: cover;">
                </div>
                <div class="carousel-buttons">
                    <button id="prev2">❮</button>
                    <button id="next2">❯</button>
                </div>
            </div>

        </section>
        <section class="section section-other">
            <h2>Other Style Story Result</h2>
            <!-- <div class="abs">MovieDreamer is able to preserve character identity over long time spans in a zero-shot manner.</div> -->
            <div class="add-div">
                <image src="images/images/comic8.png" style="width: 100%;"/>
            </div>
        </section>
        <!-- <section class="section section-other">
            <h2>Methodology</h2>
            <div class="abs">
                Inspired by movie industry, we design the multimodal script which helps the model to better understand the character, the scene, and the plot.
            </div>
            <div class="abs">
                Our autoregressive model takes multimodal scripts as input and predicts the tokens for keyframes. These tokens are then rendered into images, forming anchor frames for extended video generation. For more details, please refer to our paper.
            </div>
            <div class="add-div" style="height: 440px;">
                <image src="images/Method.png" height="440"/>
            </div>
        </section>
        <section class="section section-other">
            <h2>Compared with Existing Methods</h2>
            <div class="abs">
                Firstly, our generation paradigm can produce rich, narrative content, significantly surpassing existing methods in terms of duration. The long content we generate is not simply looping. Secondly, quantitative metrics robustly demonstrate that our method ensures high-quality results while generating lengthy content.
            </div>

            <div class="add-div" style="height: 245px;">
                <image src="images/story_metrics.png" height="245"/>
            </div>
            <div class="add-div" style="height: 200px; text-align: center ">
                <image src="images/video_metrics.png" height="200"/>
            </div>
        </section>

        <section class="section section-other">
            <h2>Bibtex</h2>
            <div>
                <pre>
                
@misc{zhao2024moviedreamer, 
    title  =  {MovieDreamer: Hierarchical Generation for Coherent Long Visual Sequence}, 
    author =  {Canyu Zhao and Mingyu Liu and Wen Wang and Jianlong Yuan and Hao Chen and Bo Zhang and Chunhua Shen},
    year   =  {2024},
    url    =  {https://arxiv.org/abs/2407.16655}, 
}
                    
                </pre>
            </div>
        </section> -->
    </div>
    <script>
        const carouselInner = document.querySelector('.carousel-inner');
        const images = document.querySelectorAll('.carousel img');
        let currentIndex = 0;
        let autoPlayInterval;
    
        function showImage(index) {
            const offset = -index * 100; // Assuming each image takes 100% of the container width
            carouselInner.style.transform = `translateX(${offset}%)`;
        }
    
        function nextImage() {
            currentIndex = (currentIndex < images.length - 1) ? currentIndex + 1 : 0;
            showImage(currentIndex);
            resetAutoPlay();
        }
    
        function prevImage() {
            currentIndex = (currentIndex > 0) ? currentIndex - 1 : images.length - 1;
            showImage(currentIndex);
            resetAutoPlay();
        }
    
        function resetAutoPlay() {
            clearInterval(autoPlayInterval);
            autoPlayInterval = setInterval(nextImage, 8000);
        }
    
        document.getElementById('prev').addEventListener('click', prevImage);
        document.getElementById('next').addEventListener('click', nextImage);
    
        // Initialize auto-play
        autoPlayInterval = setInterval(nextImage, 8000);
    
        // Seamless transition between the last and first image
        carouselInner.addEventListener('transitionend', () => {
            if (currentIndex === images.length) {
                carouselInner.style.transition = 'none';
                currentIndex = 0;
                showImage(currentIndex);
                requestAnimationFrame(() => {
                    carouselInner.style.transition = 'transform 1s ease';
                });
            }
            if (currentIndex === -1) {
                carouselInner.style.transition = 'none';
                currentIndex = images.length - 1;
                showImage(currentIndex);
                requestAnimationFrame(() => {
                    carouselInner.style.transition = 'transform 1s ease';
                });
            }
        });
    </script>


<script>
    const carouselInner1 = document.querySelector('.carousel-inner1');
    const images1 = document.querySelectorAll('.carousel1 img');
    let currentIndex1 = 0;
    let autoPlayInterval1;

    function showImage1(index) {
        const offset = -index * 100; // Assuming each image takes 100% of the container width
        carouselInner1.style.transform = `translateX(${offset}%)`;
    }

    function nextImage1() {
        console.log("hahahahahahaha", 'asdasdasd')
        currentIndex1 = (currentIndex1 < images1.length - 1) ? currentIndex1 + 1 : 0;
        showImage1(currentIndex1);
        resetAutoPlay1();
    }

    function prevImage1() {
        console.log("hahahahahaha2222222222")
        currentIndex1 = (currentIndex1 > 0) ? currentIndex1 - 1 : images1.length - 1;
        showImage1(currentIndex1);
        resetAutoPlay1();
    }

    function resetAutoPlay1() {
        clearInterval(autoPlayInterval1);
        autoPlayInterval1 = setInterval(nextImage1, 8000);
    }

    document.getElementById('prev1').addEventListener('click', prevImage1);
    document.getElementById('next1').addEventListener('click', nextImage1);

    // Initialize auto-play
    autoPlayInterval1 = setInterval(nextImage1, 8000);

    // Seamless transition between the last and first image
    carouselInner1.addEventListener('transitionend', () => {
        if (currentIndex1 === images1.length) {
            carouselInner1.style.transition = 'none';
            currentIndex1 = 0;
            showImage1(currentIndex1);
            requestAnimationFrame(() => {
                carouselInner1.style.transition = 'transform 1s ease';
            });
        }
        if (currentIndex1 === -1) {
            carouselInner1.style.transition = 'none';
            currentIndex1 = images1.length - 1;
            showImage1(currentIndex1);
            requestAnimationFrame(() => {
                carouselInner1.style.transition = 'transform 1s ease';
            });
        }
    });
</script>


<script>
    const carouselInner2 = document.querySelector('.carousel-inner2');
    const images2 = document.querySelectorAll('.carousel2 img');
    let currentIndex2 = 0;
    let autoPlayInterval2;

    function showImage2(index) {
        const offset = -index * 100; // Assuming each image takes 100% of the container width
        carouselInner2.style.transform = `translateX(${offset}%)`;
    }

    function nextImage2() {
        console.log("hahahahahahaha", 'asdasdasd')
        currentIndex2 = (currentIndex2 < images2.length - 1) ? currentIndex2 + 1 : 0;
        showImage2(currentIndex2);
        resetAutoPlay2();
    }

    function prevImage2() {
        console.log("hahahahahaha2222222222")
        currentIndex2 = (currentIndex2 > 0) ? currentIndex2 - 1 : images2.length - 1;
        showImage2(currentIndex2);
        resetAutoPlay2();
    }

    function resetAutoPlay2() {
        clearInterval(autoPlayInterval2);
        autoPlayInterval2 = setInterval(nextImage2, 8000);
    }

    document.getElementById('prev2').addEventListener('click', prevImage2);
    document.getElementById('next2').addEventListener('click', nextImage2);

    // Initialize auto-play
    autoPlayInterval2 = setInterval(nextImage2, 8000);

    // Seamless transition between the last and first image
    carouselInner2.addEventListener('transitionend', () => {
        if (currentIndex2 === images2.length) {
            carouselInner2.style.transition = 'none';
            currentIndex2 = 0;
            showImage2(currentIndex2);
            requestAnimationFrame(() => {
                carouselInner2.style.transition = 'transform 1s ease';
            });
        }
        if (currentIndex2 === -1) {
            carouselInner2.style.transition = 'none';
            currentIndex2 = images2.length - 1;
            showImage2(currentIndex2);
            requestAnimationFrame(() => {
                carouselInner2.style.transition = 'transform 1s ease';
            });
        }
    });
</script>
</body>
</html>
